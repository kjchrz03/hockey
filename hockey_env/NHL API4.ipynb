{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7842e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pprint \n",
    "import pytz\n",
    "#import schedule\n",
    "import time\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "pd.options.mode.chained_assignment = None \n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, RFE, mutual_info_classif\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f75b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# goal_counts_df = 'C:\\\\Users\\\\Karoline Sears\\\\Documents\\\\GitHub\\\\hockey-streamlit\\\\data\\\\goal_counts.csv'\n",
    "# goal_counts_df = pd.read_csv(goal_counts_df)\n",
    "# goal_location_df = 'C:\\\\Users\\\\Karoline Sears\\\\Documents\\\\GitHub\\\\hockey-streamlit\\\\data\\\\goal_locations.csv'\n",
    "# goal_location_df= pd.read_csv(goal_location_df)\n",
    "formatted_date = \"2024-04-17\"\n",
    "base_url = \"https://api-web.nhle.com/v1/schedule/\"\n",
    "# api_url = f\"{base_url}{formatted_date}\"\n",
    "api_url = \"https://api-web.nhle.com/v1/schedule/2023-10-11\"\n",
    "response = requests.get(api_url )\n",
    "content = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c5600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Grabbing game info from the 2023-2024 season\n",
    "\n",
    "# # Initialize the starting and ending game IDs\n",
    "# start_game_id = 2023020001\n",
    "# end_game_id = 2023021307\n",
    "\n",
    "# # Base URL for the API\n",
    "# base_url = \"https://api.nhle.com/stats/rest/en/shiftcharts?cayenneExp=gameId=\"\n",
    "\n",
    "# # Create an empty list to store the game IDs and URLs\n",
    "# game_data = []\n",
    "\n",
    "# # Loop through the range of game IDs\n",
    "# for game_id in range(start_game_id, end_game_id + 1):\n",
    "#     # Create the full API URL\n",
    "#     api_url = f\"{base_url}{game_id}\"\n",
    "#     # Append the game ID and URL to the list\n",
    "#     game_data.append({\"game_id\": game_id, \"url\": api_url})\n",
    "\n",
    "# # Convert the list to a DataFrame\n",
    "# game_df = pd.DataFrame(game_data)\n",
    "# game_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe77bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://api.nhle.com/stats/rest/en/shiftcharts?cayenneExp=gameId=2023020001\"\n",
    "#url = \"https://api-web.nhle.com/v1/gamecenter/2023020260/boxscore\"\n",
    "#game_id = 2023021272\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # The response content can be accessed using response.text\n",
    "    response_text = response.text\n",
    "    #pprint(response_text)\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "json_data = json.loads(response_text)\n",
    "# json_data.keys()\n",
    "events = json_data['gameWeek']\n",
    "events_df = pd.DataFrame(events)\n",
    "events_df.head()\n",
    "# events_df = events_df.convert_dtypes()\n",
    "# details = pd.json_normalize(events_df['details'])\n",
    "# details = details[['eventOwnerTeamId', 'xCoord', 'yCoord', 'shotType', 'scoringPlayerId', 'shootingPlayerId']]\n",
    "# all_game_data = pd.concat([events_df[['sortOrder', 'situationCode', 'timeInPeriod', 'typeCode']], details], axis=1)\n",
    "\n",
    "#         # Filter rows where 'typeCode' is 505-508\n",
    "# shots = all_game_data[all_game_data['typeCode'].isin([505, 506, 507, 508])]\n",
    "# shots['game_id']= game_id\n",
    "# shots\n",
    "\n",
    "# details = pd.DataFrame(json_data['data'])\n",
    "# details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced08f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "details['typeCode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7732fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "details.query('typeCode == 505').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # The response content can be accessed using response.text\n",
    "    response_text = response.text\n",
    "    #pprint(response_text)\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "json_data = json.loads(response_text)\n",
    "\n",
    "json_data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the dates and game numbers for the season\n",
    "game_dates = json_data['gameWeek']\n",
    "game_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the game info dictionary into a dataframe that shows the date, day of week abbrev, the number of games that night and a nested table of games\n",
    "df_game_info= pd.DataFrame(game_dates)\n",
    "df_game_info = df_game_info[df_game_info['numberOfGames'] != 0]\n",
    "#df_game_info = df_game_info.convert_dtypes()\n",
    "df_game_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize (separates the data in the curly brackets out into separate columns) the data in the games column to see the games\n",
    "#creates a table where every row is each date from the week and each cell contains game info\n",
    "game_data =pd.json_normalize(df_game_info['games'])\n",
    "game_data= pd.DataFrame(game_data)\n",
    "game_data.head()\n",
    "# #end_value = len(game_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to extract the data properly, it needs to be stored in a dictionary. This extracts each of the games from the cells\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "# Loop through the iterations (30 times)\n",
    "for i in range(0, len(game_data.columns)): \n",
    "    api_response = game_dates\n",
    "    \n",
    "    if api_response is not None:\n",
    "        # Extract relevant data from the API response and normalize it\n",
    "        game_info = pd.json_normalize(game_data[i])\n",
    "        \n",
    "        # Create a DataFrame for this iteration\n",
    "        df_name = f'game_test{i}'  # Generate a unique variable name\n",
    "        dfs[df_name] = pd.DataFrame(game_info)\n",
    "    else:\n",
    "        # Handle the case where the API request failed\n",
    "        print(f\"API request failed for index {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then I combine all of the dfs in the list by concatenation to create a single df. now all of the game data is spread out across each row. \n",
    "combined_df = pd.concat(dfs.values(), ignore_index=True)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.dropna(how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3dd1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "combined_df = combined_df[['id', 'season', 'startTimeUTC', 'gameType', 'awayTeam.id', 'awayTeam.abbrev', 'awayTeam.logo', \n",
    "                        'homeTeam.id', 'homeTeam.abbrev','homeTeam.logo', 'homeTeam.placeName.default', 'awayTeam.placeName.default',\n",
    "                        'awayTeam.score', 'homeTeam.score', 'winningGoalScorer.playerId', \n",
    "                        'winningGoalie.playerId', 'gameState']]\n",
    "combined_df=combined_df.convert_dtypes()\n",
    "combined_df['id'] = combined_df['id'].astype(str)\n",
    "combined_df['link_prefix'] = 'https://api-web.nhle.com/v1/gamecenter/'\n",
    "combined_df['link_suffix'] = '/play-by-play'\n",
    "combined_df['link']=combined_df['link_prefix'] + combined_df['id'] + combined_df['link_suffix']\n",
    "\n",
    "# Assuming '<NA>' is a string, replace it with np.nan\n",
    "combined_df['id'] = combined_df['id'].replace('<NA>', np.nan)\n",
    "\n",
    "# Drop rows with NaN values in the 'link' column\n",
    "combined_df = combined_df.dropna(subset=['id'])\n",
    "combined_df = combined_df.query('gameState == \"OFF\"')\n",
    "combined_df['startTimeUTC'] = pd.to_datetime(combined_df['startTimeUTC'])\n",
    "combined_df = combined_df.rename(columns = {'id':'game_id'})\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b98039",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Specify the UTC time zone\n",
    "utc_timezone = pytz.utc\n",
    "\n",
    "# Specify the target time zone (Eastern Time)\n",
    "eastern_timezone = pytz.timezone('America/New_York')\n",
    "\n",
    "# Convert 'startTimeUTC' to Eastern Time\n",
    "combined_df['game_date'] = combined_df['startTimeUTC'].dt.tz_convert(eastern_timezone)\n",
    "combined_df['game_date'] = pd.to_datetime(combined_df['game_date'])\n",
    "combined_df['game_date'] = combined_df['game_date'].dt.strftime('%Y-%m-%d')\n",
    "combined_df.drop('startTimeUTC', axis=1, inplace=True)\n",
    "# combined_df = combined_df[combined_df['game_date'] == formatted_date]\n",
    "\n",
    "# print(\"combined_df done\")\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccca497",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.sort_values(by='game_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "home_df = pd.DataFrame({\n",
    "    'game_id': combined_df['game_id'],\n",
    "    'team_id': combined_df['homeTeam.id'],\n",
    "    'tri_code': combined_df['homeTeam.abbrev'],\n",
    "    'team_name': combined_df['homeTeam.placeName.default'],\n",
    "    'value': \"home\",\n",
    "    'team_logo': combined_df['homeTeam.logo']\n",
    "})\n",
    "\n",
    "away_df =  pd.DataFrame({\n",
    "    'game_id': combined_df['game_id'],\n",
    "    'team_id': combined_df['awayTeam.id'],\n",
    "    'tri_code': combined_df['awayTeam.abbrev'],\n",
    "    'team_name': combined_df['awayTeam.placeName.default'],\n",
    "    'value':\"away\",\n",
    "    'team_logo':combined_df['awayTeam.logo']\n",
    "})    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ac090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Roster Data\n",
    "team_url = \"https://api.nhle.com/stats/rest/en/team\"\n",
    "\n",
    "response = requests.get(team_url )\n",
    "content = json.loads(response.content)\n",
    "\n",
    "# Send an HTTP GET request to the specified URL\n",
    "response = requests.get(team_url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # The response content can be accessed using response.text\n",
    "    response_text = response.text\n",
    "    #pprint(response_text)\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "json_data = json.loads(response_text)\n",
    "# json_data.keys()\n",
    "\n",
    "roster = json_data['data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "roster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf9c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_roster= pd.DataFrame(roster)\n",
    "df_roster = df_roster.convert_dtypes()\n",
    "df_roster = df_roster[['id', 'fullName', 'triCode']]\n",
    "df_roster['prefix'] = \"https://api-web.nhle.com/v1/roster/\"\n",
    "df_roster['suffix'] = \"/20232024\"\n",
    "df_roster['roster_url'] = df_roster['prefix'] + df_roster['triCode'] + df_roster['suffix']\n",
    "df_roster = df_roster[['id','fullName', 'triCode', 'roster_url']]\n",
    "df_roster = df_roster.rename(columns = {'id':'team_id', 'fullName':'team_name', 'triCode':'tri_code'})\n",
    "df_roster=df_roster.sort_values(by='team_id')\n",
    "\n",
    "\n",
    "valid_team_codes = set(range(1, 11)).union(set(range(12, 27))).union(set(range(28, 31))).union(set(range(52, 53))).union(set(range(54, 56))).union(set(range(59, 60)))\n",
    "filtered_rosters = df_roster[df_roster['team_id'].isin(valid_team_codes)]\n",
    "filtered_rosters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202889c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_rosters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty dictionary to store the results\n",
    "roster_dict = {}\n",
    "roster_link = filtered_rosters['roster_url'] \n",
    "\n",
    "    # Define your extraction script as a function\n",
    "def extract_roster_data(roster_link):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the game URL\n",
    "        response = requests.get(roster_link)\n",
    "        \n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            response_text = response.text\n",
    "            json_data = json.loads(response_text)\n",
    "\n",
    "            forwards = json_data['forwards']\n",
    "            forwards_df = pd.DataFrame(forwards)\n",
    "            forwards_df = forwards_df.convert_dtypes()\n",
    "            forwards_df = forwards_df[['id', 'headshot', 'firstName', 'lastName', 'sweaterNumber', 'positionCode']]\n",
    "            forwards_df = forwards_df.dropna(subset=['sweaterNumber'])\n",
    "\n",
    "            # Normalize (separates the data in the curly brackets out into separate columns)\n",
    "            forwards_last = pd.json_normalize(forwards_df['lastName'])\n",
    "            forwards_last = pd.DataFrame(forwards_last)\n",
    "            forwards_last = forwards_last[['default']]\n",
    "            forwards_last = forwards_last.rename(columns={'default': 'last_name'})\n",
    "\n",
    "            forwards_first = pd.json_normalize(forwards_df['firstName'])\n",
    "            forwards_first = pd.DataFrame(forwards_first)\n",
    "            forwards_first = forwards_first[['default']]\n",
    "            forwards_first = forwards_first.rename(columns={'default': 'first_name'})\n",
    "\n",
    "            forwards_roster = forwards_df.merge(forwards_first, left_index=True, right_index=True).merge(forwards_last, left_index=True, right_index=True)\n",
    "            forwards_roster['player_name'] = forwards_roster['first_name'] + \" \" + forwards_roster['last_name']\n",
    "            forwards_roster = forwards_roster[['id', 'headshot', 'player_name', 'sweaterNumber', 'positionCode']]\n",
    "\n",
    "            defense = json_data['defensemen']\n",
    "            defense_df = pd.DataFrame(defense)\n",
    "            defense_df = defense_df.convert_dtypes()\n",
    "            defense_df = defense_df[['id', 'headshot', 'firstName', 'lastName', 'sweaterNumber', 'positionCode']]\n",
    "            defense_df = defense_df.dropna(subset=['sweaterNumber'])\n",
    "\n",
    "            # Normalize (separates the data in the curly brackets out into separate columns)\n",
    "            defense_last = pd.json_normalize(defense_df['lastName'])\n",
    "            defense_last = pd.DataFrame(defense_last)\n",
    "            defense_last = defense_last[['default']]\n",
    "            defense_last = defense_last.rename(columns={'default': 'last_name'})\n",
    "\n",
    "            defense_first = pd.json_normalize(defense_df['firstName'])\n",
    "            defense_first = pd.DataFrame(defense_first)\n",
    "            defense_first = defense_first[['default']]\n",
    "            defense_first = defense_first.rename(columns={'default': 'first_name'})\n",
    "\n",
    "            defense_roster = defense_df.merge(defense_first, left_index=True, right_index=True).merge(defense_last, left_index=True, right_index=True)\n",
    "            defense_roster['player_name'] = defense_roster['first_name'] + \" \" + defense_roster['last_name']\n",
    "            defense_roster = defense_roster[['id', 'headshot', 'player_name', 'sweaterNumber', 'positionCode']]\n",
    "\n",
    "            goalies = json_data['goalies']\n",
    "            goalies_df = pd.DataFrame(goalies)\n",
    "            goalies_df = goalies_df.convert_dtypes()\n",
    "            goalies_df = goalies_df[['id', 'headshot', 'firstName', 'lastName', 'sweaterNumber', 'positionCode']]\n",
    "            goalies_df = goalies_df.dropna(subset=['sweaterNumber'])\n",
    "\n",
    "            # Normalize (separates the data in the curly brackets out into separate columns)\n",
    "            goalies_last = pd.json_normalize(goalies_df['lastName'])\n",
    "            goalies_last = pd.DataFrame(goalies_last)\n",
    "            goalies_last = goalies_last[['default']]\n",
    "            goalies_last = goalies_last.rename(columns={'default': 'last_name'})\n",
    "\n",
    "            goalies_first = pd.json_normalize(goalies_df['firstName'])\n",
    "            goalies_first = pd.DataFrame(goalies_first)\n",
    "            goalies_first = goalies_first[['default']]\n",
    "            goalies_first = goalies_first.rename(columns={'default': 'first_name'})\n",
    "\n",
    "            goalie_roster = goalies_df.merge(goalies_first, left_index=True, right_index=True).merge(goalies_last, left_index=True, right_index=True)\n",
    "            goalie_roster['player_name'] = goalie_roster['first_name'] + \" \" + goalie_roster['last_name']\n",
    "            goalie_roster = goalie_roster[['id', 'headshot', 'player_name', 'sweaterNumber', 'positionCode']]\n",
    "\n",
    "            # Append all data to form a single team roster\n",
    "            team_roster_df = pd.concat([forwards_roster, defense_roster, goalie_roster], axis=0, ignore_index=True)\n",
    "\n",
    "            team_roster_df['id'] = team_roster_df['id'] .astype(float, errors='ignore')\n",
    "\n",
    "                                \n",
    "            def extract_tricode(link):\n",
    "                # Use a regular expression to find the tricode\n",
    "                match = re.search(r'/([A-Z]{3})/', link)\n",
    "                if match:\n",
    "                    return match.group(1)\n",
    "                else:\n",
    "                    # Return None if no match is found\n",
    "                    return None\n",
    "\n",
    "            team_roster_df['tri_code'] = team_roster_df['headshot'].apply(extract_tricode)\n",
    "            return team_roster_df  # Return the entire processed DataFrame\n",
    "        else:\n",
    "            print(f\"Request failed for {roster_link} with status code {response.status_code}\")\n",
    "            return None  # Return None to indicate failure\n",
    "    \n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None  # Return None to indicate failure\n",
    "    \n",
    "# Loop through the rows of the filtered_rosters df\n",
    "for index, row in filtered_rosters.iterrows():\n",
    "    # Extract the API link from the current row\n",
    "    roster_link = row['roster_url']\n",
    "\n",
    "    # Run your game-specific data script and get the entire processed DataFrame\n",
    "    team_roster = extract_roster_data(roster_link)\n",
    "    \n",
    "    # Check if team_roster is not None before proceeding\n",
    "    if team_roster is not None:\n",
    "        # Add a 'game_id' column to the game_specific_data DataFrame\n",
    "        team_roster['tri_code'] = row['tri_code']\n",
    "\n",
    "        # Store the result in the dictionary with the game ID as the key\n",
    "        roster_dict[row['tri_code']] = team_roster\n",
    "    else:\n",
    "        print(f\"Skipping row {index} due to failed request or exception.\")\n",
    "\n",
    "all_rosters = pd.concat(roster_dict.values(), ignore_index=True)\n",
    "\n",
    "team_rosters = filtered_rosters[['team_id', 'team_name', 'tri_code']]\n",
    "team_rosters = team_rosters.merge(all_rosters,  on = \"tri_code\", how = \"left\")\n",
    "team_rosters = team_rosters.rename(columns= {'id':'player_id'})\n",
    "\n",
    "\n",
    "print(\"rosters done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "games = combined_df[['game_id', 'season', 'gameType', 'awayTeam.id', 'awayTeam.abbrev', 'awayTeam.logo', \n",
    "                        'homeTeam.id', 'homeTeam.abbrev','homeTeam.logo', 'awayTeam.score', 'homeTeam.score', 'winningGoalScorer.playerId', \n",
    "                        'winningGoalie.playerId', 'link']]\n",
    "games = games.sort_values(by='game_id')\n",
    "games = games.rename(columns = {'gameType':'game_type', 'awayTeam.id':'away_team_id', 'awayTeam.abbrev':'away_team_tricode', \n",
    "                    'awayTeam.logo': 'away_logo', 'homeTeam.id':'home_team_id', 'homeTeam.abbrev':'home_team_tricode', 'homeTeam.logo':'home_logo', \n",
    "                    'awayTeam.score': 'away_score', 'homeTeam.score':'home_score', 'winningGoalScorer.playerId':'winning_goal_player_id', \n",
    "                    'winningGoalie.playerId':'winning_goalie_id'})\n",
    "\n",
    "\n",
    "games = pd.merge(games, df_roster, left_on='home_team_id', right_on='team_id', how='left')\n",
    "\n",
    "# Rename columns and drop redundant columns\n",
    "games.rename(columns={'roster_url': 'home_roster_link', 'team_name':'home_team_name'}, inplace=True)\n",
    "games.drop(columns=['team_id', 'tri_code'], inplace=True)\n",
    "\n",
    "# Merge DataFrames based on away_team_code\n",
    "games = pd.merge(games, df_roster, left_on='away_team_id', right_on='team_id', how='left')\n",
    "\n",
    "# Rename columns and drop redundant columns\n",
    "games.rename(columns={'roster_url': 'away_roster_link', 'team_name':'away_team_name'}, inplace=True)\n",
    "games.drop(columns=['team_id','tri_code'], inplace=True)\n",
    "\n",
    "# Assuming '<NA>' is a string, replace it with np.nan\n",
    "games['game_id'].replace('<NA>', np.nan, inplace=True)\n",
    "# Drop rows with NaN values in the 'link' column\n",
    "games = games.dropna(subset=['game_id'])\n",
    "\n",
    "# Reset index after dropping rows\n",
    "games.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564dc586",
   "metadata": {},
   "outputs": [],
   "source": [
    "games.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ebb3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty dictionary to store the results\n",
    "for index, row in games.iterrows():\n",
    "    # Check if 'link' is in the columns of the current row\n",
    "    if 'link' not in row.index:\n",
    "        print(f\"Skipping row {index} as it does not contain 'link' column.\")\n",
    "        continue\n",
    "\n",
    "game_link = row['link']\n",
    "game_data_dict = {}\n",
    "game_id = row['game_id']\n",
    "\n",
    "    # Define your extraction script as a function\n",
    "def extract_game_data(game_link):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the game URL\n",
    "        response = requests.get(game_link)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            response_text = response.text\n",
    "            json_game_data = json.loads(response_text)\n",
    "\n",
    "            #event data\n",
    "            event_data = json_game_data['plays']\n",
    "            events_df = pd.DataFrame(event_data)\n",
    "            events_df = events_df.convert_dtypes()\n",
    "\n",
    "            #game events data\n",
    "            game_events = events_df[['sortOrder','timeInPeriod', 'homeTeamDefendingSide', 'typeCode']]\n",
    "\n",
    "            #coordinates data\n",
    "            details = pd.json_normalize(events_df['details'])\n",
    "            details = pd.DataFrame(details)\n",
    "            details = details[['eventOwnerTeamId', 'xCoord', 'yCoord', 'shotType', 'scoringPlayerId']]\n",
    "\n",
    "            #join the dfs\n",
    "            all_game_data = game_events.merge(details, left_index=True, right_index=True)\n",
    "\n",
    "            # Filter rows where 'typeCode' is 505\n",
    "            shots = all_game_data[all_game_data['typeCode'].isin([505, 506, 507, 508])]\n",
    "\n",
    "            # Convert the 'player_id' column to the appropriate data type (e.g., int)\n",
    "            #all_game_data['scoringPlayerId'] = all_game_data['scoringPlayerId'].astype(float).astype(pd.Int64Dtype(), errors='ignore')\n",
    "\n",
    "            # Create a 'game_id' column\n",
    "            all_game_data['game_id'] = row['game_id']\n",
    "\n",
    "            return shots  # Return the processed DataFrame and game_id\n",
    "\n",
    "        else:\n",
    "            print(f\"Request failed for {game_link} with status code {response.status_code}\")\n",
    "            return pd.DataFrame(), None  # Return an empty DataFrame and None to indicate failure\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame(), None  # Return an empty DataFrame and None to indicate failure\n",
    "    \n",
    "    # Run your game-specific data script and get the entire processed DataFrame and game_id\n",
    "for index, row in games.iterrows():\n",
    "    # Extract the API link from the current row\n",
    "    game_link = row['link']\n",
    "    \n",
    "# Run your game-specific data script and get the entire processed DataFrame\n",
    "    game_specific_data = extract_game_data(game_link)\n",
    "\n",
    "        # Add a 'game_id' column to the game_specific_data DataFrame\n",
    "    game_specific_data['game_id'] = row['game_id']\n",
    "    \n",
    "    # Store the result in the dictionary with the game ID as the key\n",
    "    game_data_dict[row['game_id']] = game_specific_data\n",
    "\n",
    "game_data = pd.concat(game_data_dict.values(), ignore_index=True)\n",
    "game_goals_data = game_data[game_data['typeCode'] == 505]\n",
    "game_goals_data = game_goals_data.rename(columns = {'scoringPlayerId':'player_id'})\n",
    "\n",
    "team_rosters = team_rosters.rename(columns = {'sweaterNumber': 'jersey_number', 'positionCode':'position'})\n",
    "\n",
    "print(\"goals done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a52be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a82ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_specific_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shots_df = game_specific_data[game_specific_data['typeCode'].isin([505, 506, 507, 508])]\n",
    "shots_df = shots_df.sort_values(by=['timeInPeriod', 'sortOrder'])\n",
    "\n",
    "shots_df=shots_df.rename(columns = {'eventOwnerTeamId':'team_id'})\n",
    "shots_df = shots_df.dropna(subset=['team_id'])\n",
    "shots_df['team_id'] =shots_df['team_id'].astype('Int64').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baa7f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7caa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Merging game data and roster data to create goal counts\n",
    "if 'player_id' in team_rosters.columns:\n",
    "    # Merge the coordinates data from final_game_data to goal_counts based on player IDs\n",
    "    goal_counts = team_rosters.merge(game_goals_data[['player_id', 'sortOrder']],on = 'player_id')\n",
    "else:\n",
    "    print(\"Column 'player_id 'does not exist in goal_counts DataFrame.\")\n",
    "\n",
    "goal_counts = goal_counts.groupby(['jersey_number','player_id', 'player_name', 'position', 'team_name'])['sortOrder'].count()\n",
    "goal_counts=pd.DataFrame(goal_counts).reset_index()\n",
    "goal_counts = goal_counts.rename(columns={'sortOrder': 'goals'})\n",
    "goal_counts = goal_counts.sort_values(by='player_name')\n",
    "goal_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_goals_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c7f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'player_id' in team_rosters.columns:\n",
    "    # Merge the coordinates data from final_game_data to goal_counts based on player IDs\n",
    "    goal_locations = team_rosters.merge(game_goals_data[['player_id', 'sortOrder', 'xCoord','yCoord', 'game_id']],on = 'player_id')\n",
    "else:\n",
    "    print(\"Column 'player_id 'does not exist in goal_counts DataFrame.\")\n",
    "\n",
    "goal_locations = goal_locations[['jersey_number', 'player_id','player_name', 'position', 'team_name', 'sortOrder', 'xCoord', 'yCoord', 'game_id']]\n",
    "goal_locations = goal_locations.dropna(subset=['player_id'])\n",
    "goal_locations = goal_locations.sort_values(['player_name', 'player_id', 'sortOrder'], ascending=[True, True, True])\n",
    "goal_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe26f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#append the data to the master set\n",
    "goal_location_df = pd.concat([goal_location_df, goal_locations], ignore_index=True)\n",
    "goal_location_df =  goal_location_df.sort_values(['player_name', 'player_id', 'sortOrder'], ascending=[True, True, True])\n",
    "\n",
    "# file_path1 = 'C:\\\\Users\\\\Karoline Sears\\\\Documents\\\\GitHub\\\\hockey-streamlit\\\\data\\\\goal_locations.csv'\n",
    "# goal_location_df.to_csv(file_path1,  index=False, encoding='utf-8')\n",
    "\n",
    "# Append last_nights_goals to season_goals\n",
    "combined_goals = pd.concat([goal_counts_df, goal_counts], ignore_index=True)\n",
    "\n",
    "# Sum up the goals for duplicated players\n",
    "result = combined_goals.groupby(['jersey_number', 'player_id', 'player_name', 'position', 'team_name'], as_index=False)['goals'].sum()\n",
    "result=result.sort_values(by='player_name')\n",
    "\n",
    "file_path2 = 'C:\\\\Users\\\\Karoline Sears\\\\Documents\\\\GitHub\\\\hockey-streamlit\\\\data\\\\goal_counts.csv'\n",
    "result.to_csv(file_path2,  index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb66c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#running count of goal number for each player - make sure this is ordered by game id and time in period\n",
    "\n",
    "goal_talley =goal_location_df.sort_values(['player_name', 'player_id', 'game_id', 'sortOrder', 'period'], \n",
    "                                          ascending=[True, True, True, True, True])\n",
    "goal_talley['goal_no'] = goal_talley.groupby('player_id').cumcount()+1\n",
    "\n",
    "def adjust_coordinates(row):\n",
    "    x = row['xCoord']\n",
    "    y = row['yCoord']\n",
    "    if x < 0:\n",
    "        adj_x = abs(x)\n",
    "        adj_y = -y\n",
    "    else:\n",
    "        adj_x = x\n",
    "        adj_y = y\n",
    "    return pd.Series({'x_adjusted': adj_x, 'y_adjusted': adj_y})\n",
    "\n",
    "# Apply the function to each row of the DataFrame\n",
    "goal_talley[['x_adjusted', 'y_adjusted']] = goal_talley.apply(adjust_coordinates, axis=1)\n",
    "ice_map_data = goal_talley\n",
    "ice_map_data=ice_map_data[['game_id', 'goal_no', 'player_id','player_name', 'team_name', 'period', 'x_adjusted', 'y_adjusted' ]]\n",
    "\n",
    "# file_path3 = 'C:\\\\Users\\\\Karoline Sears\\\\Documents\\\\GitHub\\\\hockey-streamlit\\\\data\\\\ice_map_data.csv'\n",
    "# ice_map_data.to_csv(file_path3,  index=False, encoding='utf-8')\n",
    "print(\"completed\", formatted_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
